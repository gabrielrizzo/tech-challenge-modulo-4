#  An√°lise de Sentimentos em √Åudios de Consultas Psicol√≥gicas

> **IA para Dev ‚Äî Fase 4 | Tech Challenge FIAP**

API REST multimodal que recebe √°udios de consultas psicol√≥gicas, transcreve o conte√∫do, classifica emo√ß√µes na fala e gera an√°lises psicol√≥gicas n√£o-diagn√≥sticas estruturadas em JSON ‚Äî tudo orquestrado via LangChain.

## Equipe

| Nome | RM |
|------|-----|
| Bruno Amorim | RM365279 |
| Gabriel Rizzo | RM366033 |
| Mauricio Magnani | RM365929 |
| Vinicius Martins | RM365278 |
| Gerson Luiz | RM366284 |

---

## Arquitetura

A solu√ß√£o √© uma **API REST** desenvolvida com Flask (Python 3.10+), exposta na porta `5001`, que recebe √°udios codificados em base64 e executa um pipeline multimodal de an√°lise. A orquestra√ß√£o dos modelos de IA √© feita via **LangChain**, com chamadas a provedores externos (OpenRouter) e infer√™ncia local (HuggingFace Transformers + PyTorch).

### Pipeline Principal

O endpoint central `/analyse-patient-psychological-issue` orquestra tr√™s m√≥dulos em sequ√™ncia:

1. **Transcri√ß√£o** ‚Äî O √°udio em base64 √© enviado ao modelo GPT-4o Audio Preview (via OpenRouter), que retorna o texto transcrito no idioma original.

2. **Classifica√ß√£o de Emo√ß√£o** ‚Äî O √°udio √© processado pelo modelo Whisper Large V3 fine-tuned para Speech Emotion Recognition (`firdhokk/speech-emotion-recognition-with-openai-whisper-large-v3`), usando `librosa` para pr√©-processamento e PyTorch para infer√™ncia local, retornando o label da emo√ß√£o detectada (`angry`, `sad`, `fearful`, `neutral`, etc.).

3. **An√°lise Psicol√≥gica** ‚Äî A transcri√ß√£o (texto) e a emo√ß√£o (label) s√£o combinadas e enviadas ao GPT-4o via LangChain `PromptTemplate`, gerando uma an√°lise psicol√≥gica n√£o-diagn√≥stica estruturada em JSON.

### Fluxo de Dados

```
√Åudio (base64) ‚Üí Transcri√ß√£o (texto) + Classifica√ß√£o (emo√ß√£o)
                         ‚Üì
              An√°lise Psicol√≥gica (JSON estruturado)
```

O resultado final agrega as tr√™s sa√≠das em um √∫nico payload JSON contendo: `transcription`, `emotion` e `resume` (an√°lise completa).

---

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

- **Instalar as depend√™ncias definidas em `pyproject.toml`/`uv.lock`**:

```bash
uv sync
```

Isso criar√° (ou utilizar√°) um ambiente virtual e instalar√° todas as depend√™ncias do projeto.

### Ativar o ambiente virtual criado pelo `uv`

```bash
source .venv/bin/activate
```

Se estiver no Windows (PowerShell):

```powershell
.venv\Scripts\Activate.ps1
```

### Executar o projeto

Com o ambiente ativado na raiz do projeto:

```bash
python main.py
```

Ou, caso voc√™ prefira usar o `uv` diretamente:

```bash
uv run main.py
```

### Vari√°veis de ambiente

Crie um arquivo `.env` na raiz do projeto com as seguintes chaves:

```bash
OPEN_AI_API_KEY="sua_chave_openai"
OPENROUTER_API_KEY="sua_chave_openrouter"
```

### Frontend Web

O projeto inclui uma interface web moderna e responsiva para facilitar o uso da API.

**Acesse o frontend em:** `http://localhost:5001/frontend`

#### Funcionalidades do Frontend:
- üì§ **Upload de √°udio**: Envie arquivos MP3 ou WAV diretamente do navegador
- üéØ **Transcri√ß√£o**: Converta √°udio em texto
- üß† **An√°lise Psicol√≥gica**: An√°lise detalhada do conte√∫do
- üòä **Detec√ß√£o de Emo√ß√£o**: Identifique emo√ß√µes no √°udio
- üìö **Biblioteca de √Åudios**: Acesse e analise os √°udios de exemplo
- üé® **Design Moderno**: Interface dark com anima√ß√µes suaves

Para mais detalhes, consulte: [Front/README.md](Front/README.md)

### Endpoints da API

A API roda por padr√£o em `http://localhost:5001`.

#### Sistema

## Endpoints da FrontEnd
| M√©todo | Endpoint | Descri√ß√£o |
|--------|----------|-----------|
| GET | `/frontend` | Interface web do projeto |
| GET | `/list-audios` | Lista arquivos de √°udio dispon√≠veis |
| GET | `/audio/<filename>` | Serve arquivo de √°udio espec√≠fico |

## Endpoints da API

| M√©todo | Endpoint | Descri√ß√£o |
|--------|----------|-----------|
| `GET` | `/` | Informa√ß√µes sobre a API e endpoints dispon√≠veis |
| `GET` | `/health` | Health check com status das depend√™ncias |
| `POST` | `/transcribe-audio` | Transcreve √°udio (base64) para texto |
| `POST` | `/predict-emotion` | Classifica emo√ß√£o do √°udio via Whisper SER |
| `POST` | `/analyse-audio-psycological-issue` | An√°lise psicol√≥gica direta do √°udio |
| `POST` | `/analyse-patient-psychological-issue` | **Pipeline completo:** transcri√ß√£o + emo√ß√£o + an√°lise |

---

## Modelos Aplicados

### √Åudio ‚Üí Texto (Transcri√ß√£o)

- **Modelo:** GPT-4o Audio Preview (`openai/gpt-4o-audio-preview`)
- **Provedor:** OpenRouter (`openrouter.ai/api/v1`)
- **Integra√ß√£o:** LangChain `ChatOpenAI` com `HumanMessage` contendo content type `input_audio`
- **Temperature:** `0.0` (determin√≠stico para transcri√ß√£o fiel)

O √°udio √© enviado diretamente em base64 dentro do payload da mensagem, sem necessidade de salvar arquivos tempor√°rios.

| M√©todo | Endpoint | Descri√ß√£o |
|--------|----------|-----------|
| POST | `/transcribe-audio` | Transcreve √°udio para texto |
| POST | `/analyse-audio-psycological-issue` | An√°lise psicol√≥gica N√ÉO-DIAGN√ìSTICA de √°udio |
| POST | `/predict-emotion` | Detecta emo√ß√£o presente no √°udio |
### √Åudio ‚Üí Emo√ß√£o (Classifica√ß√£o)

- **Modelo:** `firdhokk/speech-emotion-recognition-with-openai-whisper-large-v3`
- **Base:** OpenAI Whisper Large V3 fine-tuned para Speech Emotion Recognition
- **Framework:** HuggingFace Transformers (`AutoModelForAudioClassification` + `AutoFeatureExtractor`)
- **Infer√™ncia:** PyTorch (CPU ou CUDA quando dispon√≠vel)

O pipeline utiliza `librosa` para carregar o √°udio, normaliza para at√© 30 segundos com padding, e o modelo retorna logits mapeados para labels de emo√ß√£o (`angry`, `sad`, `fearful`, `neutral`, `happy`, etc.) via argmax.

### Texto + Emo√ß√£o ‚Üí An√°lise Psicol√≥gica

- **Modelo:** GPT-4o (`openai/gpt-4o` via OpenRouter)
- **Integra√ß√£o:** LangChain `PromptTemplate` com chain (`prompt | llm`)
- **Temperature:** `0.5` (balanceamento entre criatividade e consist√™ncia)
- **Output:** JSON estruturado com `response_format: json_object`

O prompt gera an√°lises n√£o-diagn√≥sticas com regras r√≠gidas: linguagem cautelosa, evid√™ncias primeiro, triagem de risco, disclaimer obrigat√≥rio e score de confiabilidade.

### Clientes Configurados

O projeto utiliza dois provedores de LLM: **OpenAI direta** (GPT-3.5-turbo como fallback) e **OpenRouter** como gateway principal para GPT-4o e GPT-4o Audio Preview. Todos configurados via LangChain `ChatOpenAI` com vari√°veis de ambiente para as API keys.

---

## Resultados e Testes

### Dataset de Teste

| √Åudio | Emo√ß√£o Esperada | Origem |
|-------|----------------|--------|
| `pt-br-angry` | Raiva | YouTube (consulta com relato agressivo) |
| `pt-br-angry-2` | Raiva | YouTube (relato emocional intenso) |
| `pt-br-sad` | Tristeza | YouTube (relato de luto/perda) |
| `pt-br-fearful` | Medo | YouTube (relato de viol√™ncia dom√©stica) |
| `en-neutral` | Neutro | Dataset HuggingFace (consulta padr√£o) |

### Estrutura do JSON de Sa√≠da

| Campo | Descri√ß√£o |
|-------|-----------|
| `disclaimer` | Aviso obrigat√≥rio de que a an√°lise √© n√£o-diagn√≥stica |
| `text_summary` | Resumo neutro do conte√∫do transcrito (1‚Äì3 frases) |
| `observed_cues` | Lista de sinais emocionais observados com categoria e relev√¢ncia |
| `possible_interpretations` | Interpreta√ß√µes cautelosas sem r√≥tulo diagn√≥stico |
| `alternative_explanations_and_limitations` | Explica√ß√µes alternativas (m√≠n. 3 itens) |
| `risk_screening` | Triagem de risco: automutila√ß√£o, suic√≠dio, viol√™ncia, perigo iminente |
| `confiability_score` | Score 0‚Äì100 com label (`low`/`medium`/`high`) e justificativa |
| `follow_up_questions_for_clinician` | 3‚Äì8 perguntas sugeridas para o psic√≥logo |
| `recommendation` | Recomenda√ß√£o personalizada baseada na emo√ß√£o detectada |

### Anomalias Detectadas

O sistema demonstrou capacidade de identificar os seguintes padr√µes nos √°udios de teste:

- **Sinais de depress√£o p√≥s-parto** ‚Äî Detectados em √°udios com emo√ß√£o `sad`, com `observed_cues` relacionados a fadiga, desesperan√ßa e isolamento social.
- **Indicadores de viol√™ncia dom√©stica** ‚Äî Identificados em √°udios `fearful`, com triagem de risco ativada para `violence_or_imminent_danger_signals`.
- **Ansiedade e estresse agudo** ‚Äî Detectados em √°udios `angry`, com recomenda√ß√µes de t√©cnicas de respira√ß√£o e busca por apoio profissional.
- **Triagem de risco autom√°tica** ‚Äî O campo `risk_screening` classificou corretamente sinais de `self-harm` como `possible`/`likely` quando o conte√∫do indicava situa√ß√µes de perigo.

### Disclaimer e Seguran√ßa

Todas as respostas incluem um **disclaimer obrigat√≥rio** indicando que a an√°lise √© n√£o-diagn√≥stica e deve ser utilizada exclusivamente por um psic√≥logo certificado. O prompt foi projetado com regras non-negotiable para evitar r√≥tulos diagn√≥sticos e utilizar linguagem cautelosa.

---

## ‚öôÔ∏è Stack Tecnol√≥gica

| Tecnologia | Papel no Projeto |
|------------|-----------------|
| **Flask 3.1+** | Framework web (API REST) |
| **LangChain** | Orquestra√ß√£o de LLMs e pipelines de prompts |
| **OpenAI GPT-4o** | An√°lise psicol√≥gica n√£o-diagn√≥stica (via OpenRouter) |
| **GPT-4o Audio Preview** | Transcri√ß√£o de √°udio (via OpenRouter) |
| **Whisper Large V3** (fine-tuned) | Classifica√ß√£o de emo√ß√µes em fala (HuggingFace) |
| **PyTorch + Transformers** | Infer√™ncia do modelo de emo√ß√µes |
| **librosa** | Pr√©-processamento de √°udio |
| **Python 3.10+ / uv** | Runtime e gerenciamento de depend√™ncias |

---

## Artefatos

- **Reposit√≥rio GitHub:** [github.com/gabrielrizzo/tech-challenge-modulo-4](https://github.com/gabrielrizzo/tech-challenge-modulo-4)
- **Datasets HuggingFace:**
  - [TTS Dataset](https://huggingface.co/datasets/brunoretiro/saude-mulher-openai-tts)
  - [Health Dataset](https://huggingface.co/datasets/brunoretiro/womanhealthfiap)
- **V√≠deo Demonstrativo:** *(inserir link do YouTube)*

---

> **Fevereiro 2026** ‚Äî FIAP IA para Dev
