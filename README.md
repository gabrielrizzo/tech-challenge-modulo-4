#  AnÃ¡lise de Sentimentos em Ãudios de Consultas PsicolÃ³gicas

> **IA para Dev â€” Fase 4 | Tech Challenge FIAP**

API REST multimodal que recebe Ã¡udios de consultas psicolÃ³gicas, transcreve o conteÃºdo, classifica emoÃ§Ãµes na fala e gera anÃ¡lises psicolÃ³gicas nÃ£o-diagnÃ³sticas estruturadas em JSON â€” tudo orquestrado via LangChain/LangGraph.

## ðŸ‘¥ Equipe

| Nome | RM |
|------|-----|
| Bruno Amorim | RM365279 |
| Gabriel Rizzo | RM366033 |
| Mauricio Magnani | RM365929 |
| Vinicius Martins | RM365278 |
| Gerson Luiz | RM366284 |

---

## ðŸ—ï¸ Arquitetura

A soluÃ§Ã£o Ã© uma **API REST** desenvolvida com Flask (Python 3.10+), exposta na porta `5001`, que recebe Ã¡udios codificados em base64 e executa um pipeline multimodal de anÃ¡lise. A orquestraÃ§Ã£o dos modelos de IA Ã© feita via **LangChain/LangGraph**, com chamadas a provedores externos (OpenRouter) e inferÃªncia local (HuggingFace Transformers + PyTorch).

### Pipeline Principal

O endpoint central `/analyse-patient-psychological-issue` orquestra trÃªs mÃ³dulos em sequÃªncia:

1. **TranscriÃ§Ã£o** â€” O Ã¡udio em base64 Ã© enviado ao modelo GPT-4o Audio Preview (via OpenRouter), que retorna o texto transcrito no idioma original.

2. **ClassificaÃ§Ã£o de EmoÃ§Ã£o** â€” O Ã¡udio Ã© processado pelo modelo Whisper Large V3 fine-tuned para Speech Emotion Recognition (`firdhokk/speech-emotion-recognition-with-openai-whisper-large-v3`), usando `librosa` para prÃ©-processamento e PyTorch para inferÃªncia local, retornando o label da emoÃ§Ã£o detectada (`angry`, `sad`, `fearful`, `neutral`, etc.).

3. **AnÃ¡lise PsicolÃ³gica** â€” A transcriÃ§Ã£o (texto) e a emoÃ§Ã£o (label) sÃ£o combinadas e enviadas ao GPT-4o via LangChain `PromptTemplate`, gerando uma anÃ¡lise psicolÃ³gica nÃ£o-diagnÃ³stica estruturada em JSON.

### Fluxo de Dados

```
Ãudio (base64) â†’ TranscriÃ§Ã£o (texto) + ClassificaÃ§Ã£o (emoÃ§Ã£o)
                         â†“
              AnÃ¡lise PsicolÃ³gica (JSON estruturado)
```

O resultado final agrega as trÃªs saÃ­das em um Ãºnico payload JSON contendo: `transcription`, `emotion` e `resume` (anÃ¡lise completa).

---

## ðŸ“¡ Endpoints da API

| MÃ©todo | Endpoint | DescriÃ§Ã£o |
|--------|----------|-----------|
| `GET` | `/` | InformaÃ§Ãµes sobre a API e endpoints disponÃ­veis |
| `GET` | `/health` | Health check com status das dependÃªncias |
| `POST` | `/transcribe-audio` | Transcreve Ã¡udio (base64) para texto |
| `POST` | `/predict-emotion` | Classifica emoÃ§Ã£o do Ã¡udio via Whisper SER |
| `POST` | `/analyse-audio-psycological-issue` | AnÃ¡lise psicolÃ³gica direta do Ã¡udio |
| `POST` | `/analyse-patient-psychological-issue` | **Pipeline completo:** transcriÃ§Ã£o + emoÃ§Ã£o + anÃ¡lise |

---

## ðŸ¤– Modelos Aplicados

### Ãudio â†’ Texto (TranscriÃ§Ã£o)

- **Modelo:** GPT-4o Audio Preview (`openai/gpt-4o-audio-preview`)
- **Provedor:** OpenRouter (`openrouter.ai/api/v1`)
- **IntegraÃ§Ã£o:** LangChain `ChatOpenAI` com `HumanMessage` contendo content type `input_audio`
- **Temperature:** `0.0` (determinÃ­stico para transcriÃ§Ã£o fiel)

O Ã¡udio Ã© enviado diretamente em base64 dentro do payload da mensagem, sem necessidade de salvar arquivos temporÃ¡rios.

### Ãudio â†’ EmoÃ§Ã£o (ClassificaÃ§Ã£o)

- **Modelo:** `firdhokk/speech-emotion-recognition-with-openai-whisper-large-v3`
- **Base:** OpenAI Whisper Large V3 fine-tuned para Speech Emotion Recognition
- **Framework:** HuggingFace Transformers (`AutoModelForAudioClassification` + `AutoFeatureExtractor`)
- **InferÃªncia:** PyTorch (CPU ou CUDA quando disponÃ­vel)

O pipeline utiliza `librosa` para carregar o Ã¡udio, normaliza para atÃ© 30 segundos com padding, e o modelo retorna logits mapeados para labels de emoÃ§Ã£o (`angry`, `sad`, `fearful`, `neutral`, `happy`, etc.) via argmax.

### Texto + EmoÃ§Ã£o â†’ AnÃ¡lise PsicolÃ³gica

- **Modelo:** GPT-4o (`openai/gpt-4o` via OpenRouter)
- **IntegraÃ§Ã£o:** LangChain `PromptTemplate` com chain (`prompt | llm`)
- **Temperature:** `0.5` (balanceamento entre criatividade e consistÃªncia)
- **Output:** JSON estruturado com `response_format: json_object`

O prompt gera anÃ¡lises nÃ£o-diagnÃ³sticas com regras rÃ­gidas: linguagem cautelosa, evidÃªncias primeiro, triagem de risco, disclaimer obrigatÃ³rio e score de confiabilidade.

### Clientes Configurados

O projeto utiliza dois provedores de LLM: **OpenAI direta** (GPT-3.5-turbo como fallback) e **OpenRouter** como gateway principal para GPT-4o e GPT-4o Audio Preview. Todos configurados via LangChain `ChatOpenAI` com variÃ¡veis de ambiente para as API keys.

---

## ðŸ§ª Resultados e Testes

### Dataset de Teste

| Ãudio | EmoÃ§Ã£o Esperada | Origem |
|-------|----------------|--------|
| `pt-br-angry` | Raiva | YouTube (consulta com relato agressivo) |
| `pt-br-angry-2` | Raiva | YouTube (relato emocional intenso) |
| `pt-br-sad` | Tristeza | YouTube (relato de luto/perda) |
| `pt-br-fearful` | Medo | YouTube (relato de violÃªncia domÃ©stica) |
| `en-neutral` | Neutro | Dataset HuggingFace (consulta padrÃ£o) |

### Estrutura do JSON de SaÃ­da

| Campo | DescriÃ§Ã£o |
|-------|-----------|
| `disclaimer` | Aviso obrigatÃ³rio de que a anÃ¡lise Ã© nÃ£o-diagnÃ³stica |
| `text_summary` | Resumo neutro do conteÃºdo transcrito (1â€“3 frases) |
| `observed_cues` | Lista de sinais emocionais observados com categoria e relevÃ¢ncia |
| `possible_interpretations` | InterpretaÃ§Ãµes cautelosas sem rÃ³tulo diagnÃ³stico |
| `alternative_explanations_and_limitations` | ExplicaÃ§Ãµes alternativas (mÃ­n. 3 itens) |
| `risk_screening` | Triagem de risco: automutilaÃ§Ã£o, suicÃ­dio, violÃªncia, perigo iminente |
| `confiability_score` | Score 0â€“100 com label (`low`/`medium`/`high`) e justificativa |
| `follow_up_questions_for_clinician` | 3â€“8 perguntas sugeridas para o psicÃ³logo |
| `recommendation` | RecomendaÃ§Ã£o personalizada baseada na emoÃ§Ã£o detectada |

### Anomalias Detectadas

O sistema demonstrou capacidade de identificar os seguintes padrÃµes nos Ã¡udios de teste:

- **Sinais de depressÃ£o pÃ³s-parto** â€” Detectados em Ã¡udios com emoÃ§Ã£o `sad`, com `observed_cues` relacionados a fadiga, desesperanÃ§a e isolamento social.
- **Indicadores de violÃªncia domÃ©stica** â€” Identificados em Ã¡udios `fearful`, com triagem de risco ativada para `violence_or_imminent_danger_signals`.
- **Ansiedade e estresse agudo** â€” Detectados em Ã¡udios `angry`, com recomendaÃ§Ãµes de tÃ©cnicas de respiraÃ§Ã£o e busca por apoio profissional.
- **Triagem de risco automÃ¡tica** â€” O campo `risk_screening` classificou corretamente sinais de `self-harm` como `possible`/`likely` quando o conteÃºdo indicava situaÃ§Ãµes de perigo.

### Disclaimer e SeguranÃ§a

Todas as respostas incluem um **disclaimer obrigatÃ³rio** indicando que a anÃ¡lise Ã© nÃ£o-diagnÃ³stica e deve ser utilizada exclusivamente por um psicÃ³logo certificado. O prompt foi projetado com regras non-negotiable para evitar rÃ³tulos diagnÃ³sticos e utilizar linguagem cautelosa.

---

## âš™ï¸ Stack TecnolÃ³gica

| Tecnologia | Papel no Projeto |
|------------|-----------------|
| **Flask 3.1+** | Framework web (API REST) |
| **LangChain / LangGraph** | OrquestraÃ§Ã£o de LLMs e pipelines de prompts |
| **OpenAI GPT-4o** | AnÃ¡lise psicolÃ³gica nÃ£o-diagnÃ³stica (via OpenRouter) |
| **GPT-4o Audio Preview** | TranscriÃ§Ã£o de Ã¡udio (via OpenRouter) |
| **Whisper Large V3** (fine-tuned) | ClassificaÃ§Ã£o de emoÃ§Ãµes em fala (HuggingFace) |
| **PyTorch + Transformers** | InferÃªncia do modelo de emoÃ§Ãµes |
| **librosa** | PrÃ©-processamento de Ã¡udio |
| **Python 3.10+ / uv** | Runtime e gerenciamento de dependÃªncias |

---

## ðŸ”— Artefatos

- **RepositÃ³rio GitHub:** [github.com/gabrielrizzo/tech-challenge-modulo-4](https://github.com/gabrielrizzo/tech-challenge-modulo-4)
- **Datasets HuggingFace:**
  - [TTS Dataset](https://huggingface.co/datasets/brunoretiro/saude-mulher-openai-tts)
  - [Health Dataset](https://huggingface.co/datasets/brunoretiro/womanhealthfiap)
- **VÃ­deo Demonstrativo:** *(inserir link do YouTube)*

---

> **Fevereiro 2026** â€” FIAP IA para Dev
